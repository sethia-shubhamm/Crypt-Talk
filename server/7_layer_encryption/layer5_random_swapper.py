"""
ðŸ”’ Layer 5: Random Swapper (N-Round Block Permutation)
======================================================

This layer implements sophisticated block-level permutation using multiple rounds
of Fisher-Yates shuffles to completely destroy the linear order of ciphertext blocks.
This prevents pattern-based attacks and block reordering vulnerabilities.

Key Features:
- Variable N-round Fisher-Yates permutations
- Cryptographically secure round count derivation
- Fixed-size block processing (16-byte AES blocks)
- Deterministic permutation with inverse operations
- Memory-efficient in-place swapping

Security Purpose:
- Destroys block order patterns and relationships
- Prevents block reordering and cut-and-paste attacks
- Adds positional confusion to ciphertext structure
- Makes block-level analysis computationally infeasible
- Provides defense against known-block attacks

Implementation Details:
- Uses HMAC for secure round count and swap derivation
- Processes data in 16-byte blocks (AES block size)
- Handles arbitrary data lengths with padding
- Constant-time operations to prevent timing attacks
- Inverse permutation tracking for decryption
"""

import hashlib
import hmac
import secrets
import struct
from typing import List, Tuple


class RandomSwapper:
    """
    Layer 5 of 7-layer encryption: N-round block permutation system
    """
    
    # Configuration constants
    BLOCK_SIZE = 16  # AES block size for consistent processing
    MIN_ROUNDS = 3   # Minimum permutation rounds for security
    MAX_ROUNDS = 16  # Maximum rounds to prevent excessive computation
    
    def __init__(self):
        self.last_permutation = []  # Track last permutation for debugging
        
    def _derive_swap_parameters(self, master_key: bytes, nonce: bytes, 
                               layer_id: bytes = b"LAYER5_SWAPPER") -> Tuple[int, bytes]:
        """
        Derive number of swap rounds and swap sequence from key material
        
        Args:
            master_key: Master encryption key
            nonce: Unique nonce for this operation
            layer_id: Layer identifier for parameter separation
            
        Returns:
            Tuple of (num_rounds, swap_material)
        """
        # Create context for swap parameter derivation
        swap_context = layer_id + nonce + b"BLOCK_PERMUTATION"
        
        # Generate HMAC for secure parameter derivation
        h = hmac.new(master_key, swap_context, hashlib.sha256)
        param_hash = h.digest()
        
        # Derive number of rounds from first bytes
        rounds_raw = struct.unpack('<I', param_hash[:4])[0]
        num_rounds = (rounds_raw % (self.MAX_ROUNDS - self.MIN_ROUNDS + 1)) + self.MIN_ROUNDS
        
        # Generate additional random material for swaps
        # Each round needs log2(blocks) random values on average
        # Generate extra material to ensure we have enough
        swap_material = param_hash  # Start with HMAC output
        
        # Extend swap material using key stretching
        for i in range(8):  # Generate 8 additional hash blocks
            extend_context = swap_context + struct.pack('<I', i)
            h = hmac.new(master_key, extend_context, hashlib.sha256)
            swap_material += h.digest()
            
        return num_rounds, swap_material
    
    def _pad_to_blocks(self, data: bytes) -> Tuple[bytes, int]:
        """
        Pad data to multiple of block size using PKCS#7 padding
        
        Args:
            data: Input data to pad
            
        Returns:
            Tuple of (padded_data, original_length)
        """
        original_length = len(data)
        
        if original_length == 0:
            # Handle empty data case
            padding_length = self.BLOCK_SIZE
            padded_data = bytes([padding_length] * padding_length)
            return padded_data, 0
        
        # Calculate padding needed
        padding_length = self.BLOCK_SIZE - (original_length % self.BLOCK_SIZE)
        if padding_length == 0:
            padding_length = self.BLOCK_SIZE  # Always add at least one block of padding
            
        # Apply PKCS#7 padding
        padding_bytes = bytes([padding_length] * padding_length)
        padded_data = data + padding_bytes
        
        return padded_data, original_length
    
    def _unpad_blocks(self, padded_data: bytes, original_length: int) -> bytes:
        """
        Remove PKCS#7 padding from data
        
        Args:
            padded_data: Padded data to unpad
            original_length: Original data length before padding
            
        Returns:
            Unpadded data
        """
        if original_length == 0:
            return b""  # Empty original data
            
        # Validate original length
        if original_length > len(padded_data):
            raise ValueError("Invalid original length: exceeds padded data size")
            
        # Extract original data
        return padded_data[:original_length]
    
    def _split_into_blocks(self, data: bytes) -> List[bytes]:
        """
        Split data into fixed-size blocks
        
        Args:
            data: Data to split (must be multiple of BLOCK_SIZE)
            
        Returns:
            List of blocks
        """
        if len(data) % self.BLOCK_SIZE != 0:
            raise ValueError(f"Data length must be multiple of {self.BLOCK_SIZE}")
            
        blocks = []
        for i in range(0, len(data), self.BLOCK_SIZE):
            blocks.append(data[i:i + self.BLOCK_SIZE])
            
        return blocks
    
    def _join_blocks(self, blocks: List[bytes]) -> bytes:
        """
        Join blocks back into single byte string
        
        Args:
            blocks: List of blocks to join
            
        Returns:
            Concatenated data
        """
        return b''.join(blocks)
    
    def _fisher_yates_shuffle(self, blocks: List[bytes], swap_material: bytes, 
                             round_num: int) -> List[int]:
        """
        Perform Fisher-Yates shuffle on blocks and return permutation indices
        
        Args:
            blocks: List of blocks to shuffle (modified in-place)
            swap_material: Random material for swap decisions
            round_num: Current round number for material indexing
            
        Returns:
            List of original indices (for inverse permutation)
        """
        num_blocks = len(blocks)
        if num_blocks <= 1:
            return list(range(num_blocks))
            
        # Track permutation for inverse operation
        indices = list(range(num_blocks))
        
        # Calculate offset into swap material for this round
        material_offset = (round_num * num_blocks * 4) % len(swap_material)
        
        # Perform Fisher-Yates shuffle
        for i in range(num_blocks - 1, 0, -1):
            # Get random index from swap material
            if material_offset + 4 > len(swap_material):
                # Wrap around if we need more material
                material_offset = 0
                
            random_bytes = swap_material[material_offset:material_offset + 4]
            material_offset += 4
            
            random_int = struct.unpack('<I', random_bytes)[0]
            j = random_int % (i + 1)  # Random index from 0 to i (inclusive)
            
            # Swap blocks and track indices
            blocks[i], blocks[j] = blocks[j], blocks[i]
            indices[i], indices[j] = indices[j], indices[i]
            
        return indices
    
    def _inverse_fisher_yates(self, blocks: List[bytes], permutation_indices: List[int]):
        """
        Apply inverse Fisher-Yates permutation to restore original order
        
        Args:
            blocks: List of blocks to unpermute (modified in-place)
            permutation_indices: Indices from forward permutation
        """
        num_blocks = len(blocks)
        if num_blocks <= 1:
            return
            
        # Create inverse mapping
        inverse_indices = [0] * num_blocks
        for i, orig_idx in enumerate(permutation_indices):
            inverse_indices[orig_idx] = i
            
        # Apply inverse permutation
        temp_blocks = blocks[:]  # Copy current state
        for i in range(num_blocks):
            blocks[i] = temp_blocks[inverse_indices[i]]
    
    def _pack_swapped_data(self, swapped_blocks: bytes, original_length: int, 
                          permutation_data: bytes) -> bytes:
        """
        Pack swapped blocks with metadata for decryption
        
        Args:
            swapped_blocks: Permuted block data
            original_length: Original data length before padding
            permutation_data: Encoded permutation information
            
        Returns:
            Packed data structure
        """
        perm_len = len(permutation_data)
        
        if perm_len > 65535:
            raise ValueError("Permutation data too large (max 64KB)")
            
        # Pack: [orig_len:4][perm_len:2][perm_data][swapped_blocks]
        packed = bytearray()
        packed.extend(struct.pack('<I', original_length))  # 4 bytes: original length
        packed.extend(struct.pack('<H', perm_len))         # 2 bytes: permutation length
        packed.extend(permutation_data)                    # Variable: permutation data
        packed.extend(swapped_blocks)                      # Variable: swapped blocks
        
        return bytes(packed)
    
    def _unpack_swapped_data(self, packed_data: bytes) -> Tuple[bytes, int, bytes]:
        """
        Unpack swapped blocks and metadata
        
        Args:
            packed_data: Packed data structure
            
        Returns:
            Tuple of (swapped_blocks, original_length, permutation_data)
        """
        if len(packed_data) < 6:  # Minimum: 4 + 2 = 6 bytes
            raise ValueError("Invalid packed data: too short")
            
        # Extract original length
        original_length = struct.unpack('<I', packed_data[:4])[0]
        
        # Extract permutation data length
        perm_len = struct.unpack('<H', packed_data[4:6])[0]
        
        # Extract permutation data
        if len(packed_data) < 6 + perm_len:
            raise ValueError("Invalid packed data: truncated permutation data")
            
        permutation_data = packed_data[6:6 + perm_len]
        
        # Extract swapped blocks
        swapped_blocks = packed_data[6 + perm_len:]
        
        return swapped_blocks, original_length, permutation_data
    
    def encrypt(self, plaintext: bytes, master_key: bytes, nonce: bytes) -> bytes:
        """
        Apply N-round block permutation to plaintext
        
        Args:
            plaintext: Data to permute
            master_key: Master encryption key (32+ bytes)
            nonce: Unique nonce for this operation (16+ bytes)
            
        Returns:
            Permuted data with reconstruction metadata
        """
        if len(master_key) < 32:
            raise ValueError("Master key must be at least 32 bytes")
        if len(nonce) < 16:
            raise ValueError("Nonce must be at least 16 bytes")
        if not plaintext:
            raise ValueError("Plaintext cannot be empty")
            
        # Derive swap parameters
        num_rounds, swap_material = self._derive_swap_parameters(master_key, nonce)
        
        # Pad data to block boundaries
        padded_data, original_length = self._pad_to_blocks(plaintext)
        
        # Split into blocks
        blocks = self._split_into_blocks(padded_data)
        
        # Apply N rounds of Fisher-Yates permutations
        all_permutations = []
        for round_num in range(num_rounds):
            permutation = self._fisher_yates_shuffle(blocks, swap_material, round_num)
            all_permutations.append(permutation)
            
        # Store permutation history for debugging
        self.last_permutation = all_permutations
        
        # Join blocks back together
        swapped_data = self._join_blocks(blocks)
        
        # Encode permutation information
        permutation_info = struct.pack('<H', num_rounds)  # Number of rounds
        for perm in all_permutations:
            # Encode each permutation as series of indices
            for idx in perm:
                permutation_info += struct.pack('<H', idx)
                
        # Pack result with metadata
        packed_result = self._pack_swapped_data(swapped_data, original_length, permutation_info)
        
        return packed_result
    
    def decrypt(self, ciphertext: bytes, master_key: bytes, nonce: bytes) -> bytes:
        """
        Reverse N-round block permutation from ciphertext
        
        Args:
            ciphertext: Permuted data to restore
            master_key: Master encryption key (must match encryption key)
            nonce: Nonce used during encryption (must match)
            
        Returns:
            Restored plaintext with original block order
        """
        if len(master_key) < 32:
            raise ValueError("Master key must be at least 32 bytes")
        if len(nonce) < 16:
            raise ValueError("Nonce must be at least 16 bytes")
        if not ciphertext:
            raise ValueError("Ciphertext cannot be empty")
            
        # Unpack swapped data and metadata
        swapped_data, original_length, permutation_info = self._unpack_swapped_data(ciphertext)
        
        # Parse permutation information
        if len(permutation_info) < 2:
            raise ValueError("Invalid permutation info: too short")
            
        num_rounds = struct.unpack('<H', permutation_info[:2])[0]
        
        # Validate rounds count
        if num_rounds < self.MIN_ROUNDS or num_rounds > self.MAX_ROUNDS:
            raise ValueError(f"Invalid rounds count: {num_rounds}")
            
        # Split swapped data into blocks
        blocks = self._split_into_blocks(swapped_data)
        num_blocks = len(blocks)
        
        # Parse all permutations
        all_permutations = []
        info_offset = 2
        
        for round_num in range(num_rounds):
            if info_offset + (num_blocks * 2) > len(permutation_info):
                raise ValueError("Invalid permutation info: truncated data")
                
            permutation = []
            for block_idx in range(num_blocks):
                idx = struct.unpack('<H', permutation_info[info_offset:info_offset + 2])[0]
                permutation.append(idx)
                info_offset += 2
                
            all_permutations.append(permutation)
            
        # Apply inverse permutations in reverse order
        for round_num in range(num_rounds - 1, -1, -1):
            self._inverse_fisher_yates(blocks, all_permutations[round_num])
            
        # Join blocks and remove padding
        restored_data = self._join_blocks(blocks)
        plaintext = self._unpad_blocks(restored_data, original_length)
        
        return plaintext
    
    def analyze_permutation_quality(self, num_blocks: int, master_key: bytes, 
                                   nonce: bytes, num_tests: int = 1000) -> dict:
        """
        Analyze quality of block permutation for cryptographic strength
        
        Args:
            num_blocks: Number of blocks to test with
            master_key: Master key for testing
            nonce: Nonce for testing
            num_tests: Number of permutation tests to run
            
        Returns:
            Dictionary with permutation quality metrics
        """
        # Generate test data
        test_data = bytes(range(num_blocks * self.BLOCK_SIZE))
        
        position_changes = []  # Track how far each block moves
        collision_counts = []  # Track position collisions
        
        for test_num in range(num_tests):
            # Use different nonce for each test
            test_nonce = nonce + struct.pack('<I', test_num)
            
            # Encrypt (permute) test data
            encrypted = self.encrypt(test_data, master_key, test_nonce)
            
            # Analyze the permutation
            if hasattr(self, 'last_permutation') and self.last_permutation:
                final_positions = list(range(num_blocks))
                
                # Apply all rounds to see final position mapping
                for round_perm in self.last_permutation:
                    temp_positions = final_positions[:]
                    for i, orig_pos in enumerate(round_perm):
                        final_positions[orig_pos] = temp_positions[i]
                
                # Calculate position changes
                changes = [abs(final_pos - orig_pos) for orig_pos, final_pos in enumerate(final_positions)]
                position_changes.extend(changes)
                
                # Count position collisions (multiple blocks mapping to same position)
                collisions = len(final_positions) - len(set(final_positions))
                collision_counts.append(collisions)
        
        # Calculate statistics
        avg_movement = sum(position_changes) / len(position_changes) if position_changes else 0
        max_movement = max(position_changes) if position_changes else 0
        avg_collisions = sum(collision_counts) / len(collision_counts) if collision_counts else 0
        
        # Expected movement for good permutation is ~33% of block range
        expected_movement = num_blocks / 3
        movement_quality = abs(avg_movement - expected_movement) / expected_movement < 0.3
        
        return {
            "num_blocks_tested": num_blocks,
            "tests_performed": num_tests,
            "average_movement": avg_movement,
            "expected_movement": expected_movement,
            "max_movement": max_movement,
            "movement_quality": movement_quality,
            "average_collisions": avg_collisions,
            "collision_quality": avg_collisions == 0,  # Should be no collisions
            "overall_quality": "PASS" if movement_quality and avg_collisions == 0 else "FAIL"
        }


# Test and demonstration code
if __name__ == "__main__":
    import secrets
    import time
    
    # Initialize swapper
    swapper = RandomSwapper()
    
    print("ðŸ”’ Layer 5: Random Swapper Testing")
    print("=" * 50)
    
    # Generate test materials
    master_key = secrets.token_bytes(32)
    nonce = secrets.token_bytes(16)
    test_message = b"Block permutation test! This message will have its blocks completely scrambled! " * 10
    
    print(f"Original message length: {len(test_message)} bytes")
    print(f"Block size: {RandomSwapper.BLOCK_SIZE} bytes")
    print(f"Number of blocks: {(len(test_message) + RandomSwapper.BLOCK_SIZE - 1) // RandomSwapper.BLOCK_SIZE}")
    
    # Test 1: Basic permutation/restoration
    start_time = time.time()
    permuted = swapper.encrypt(test_message, master_key, nonce)
    encrypt_time = time.time() - start_time
    
    start_time = time.time()
    restored = swapper.decrypt(permuted, master_key, nonce)
    decrypt_time = time.time() - start_time
    
    print(f"\nâœ… Permutation successful: {encrypt_time:.6f}s")
    print(f"âœ… Restoration successful: {decrypt_time:.6f}s")
    print(f"âœ… Data integrity: {'PASS' if test_message == restored else 'FAIL'}")
    print(f"Permuted size: {len(permuted)} bytes (overhead: {len(permuted) - len(test_message)} bytes)")
    
    # Test 2: Analyze permutation rounds
    num_rounds, _ = swapper._derive_swap_parameters(master_key, nonce)
    print(f"\nPermutation Analysis:")
    print(f"Number of rounds: {num_rounds}")
    print(f"Rounds range: {RandomSwapper.MIN_ROUNDS}-{RandomSwapper.MAX_ROUNDS}")
    
    # Test 3: Quality analysis
    quality = swapper.analyze_permutation_quality(32, master_key, nonce, 100)
    print(f"\nPermutation Quality (32 blocks, 100 tests):")
    print(f"Average movement: {quality['average_movement']:.1f} blocks")
    print(f"Expected movement: {quality['expected_movement']:.1f} blocks")
    print(f"Movement quality: {quality['movement_quality']}")
    print(f"Collision quality: {quality['collision_quality']}")
    print(f"Overall quality: {quality['overall_quality']}")
    
    # Test 4: Different nonces produce different permutations
    nonce2 = secrets.token_bytes(16)
    permuted2 = swapper.encrypt(test_message, master_key, nonce2)
    
    nonce_independence = permuted != permuted2
    print(f"\nNonce independence: {'PASS' if nonce_independence else 'FAIL'}")
    
    # Test 5: Performance with different sizes
    print(f"\nPerformance Testing:")
    for blocks in [16, 64, 256]:  # Different block counts
        size = blocks * RandomSwapper.BLOCK_SIZE
        test_data = secrets.token_bytes(size)
        
        start_time = time.time()
        permuted = swapper.encrypt(test_data, master_key, nonce)
        encrypt_time = time.time() - start_time
        
        start_time = time.time()
        restored = swapper.decrypt(permuted, master_key, nonce)
        decrypt_time = time.time() - start_time
        
        throughput = size / (encrypt_time + decrypt_time) / 1024 / 1024  # MB/s
        print(f"{blocks:3d} blocks ({size:5d} bytes): {throughput:8.2f} MB/s")
    
    # Test 6: Edge cases
    print(f"\nEdge Case Testing:")
    
    # Single block
    single_block = b"A" * RandomSwapper.BLOCK_SIZE
    permuted_single = swapper.encrypt(single_block, master_key, nonce)
    restored_single = swapper.decrypt(permuted_single, master_key, nonce)
    single_test = single_block == restored_single
    
    # Small data (less than block size)
    small_data = b"Small"
    permuted_small = swapper.encrypt(small_data, master_key, nonce)
    restored_small = swapper.decrypt(permuted_small, master_key, nonce)
    small_test = small_data == restored_small
    
    print(f"Single block test: {'PASS' if single_test else 'FAIL'}")
    print(f"Small data test: {'PASS' if small_test else 'FAIL'}")
    
    all_tests_pass = (
        test_message == restored and 
        quality['overall_quality'] == 'PASS' and
        nonce_independence and
        single_test and
        small_test
    )
    
    print(f"\nðŸŽ¯ Layer 5 Status: {'OPERATIONAL' if all_tests_pass else 'FAILED'}")
    
    if all_tests_pass:
        print("ðŸ”„ Block permutation system successfully implemented!")
        print("   Linear order destroyed, security enhanced.")